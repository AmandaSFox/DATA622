---
title: "DATA 622 - Assignment 4 - Fox"
author: "Amanda Fox"
date: "2025-05-11"
output: html_document
---

  
## Introduction
  
This document contains all code for Assignment 4. The business goal is to predict customer churn for a fictional telecommunications company using the "Telco Customer Churn" dataset sourced from Kaggle. 

Two modeling approaches are used: random forest and neural networks, with both default and tuned versions of each. All models are evaluated on multiple metrics, which are stored in a matrix for comparison. Balanced performance is important but recall is prioritized somewhat, reflecting the desire to capture as many at-risk customers as possible due to the high cost of a lost customer. 
  
## Load libraries
  
```{r libraries, message=FALSE, warning=FALSE}

library(tidyverse)
library(fastDummies)
library(randomForest)
library(pROC)
library(caret)
library(nnet)
library(scales)
```


## Data Prep

### Load data

```{r load, message=FALSE, warning=FALSE}

# Load data
df_raw <- read_csv("https://raw.githubusercontent.com/AmandaSFox/DATA622/refs/heads/main/Assignment_4/WA_Fn-UseC_-Telco-Customer-Churn.csv")

# View raw dataset
glimpse(df_raw)
```
  
### Handle missing values and duplicate records 

This dataset is preprocessed and very clean: no duplicate records were found and only 11 NA values. All NA values were in the "Total Charges" column and associated with brand new accounts. These were removed from the dataset as not useful for churn analysis.
  
```{r clean}

# Duplicate rows
sum(duplicated(df_raw))

# Count NA values by column
colSums(is.na(df_raw)) 

# View NA data
missing_chgs <- df_raw %>% 
  filter(is.na(TotalCharges)) # All have tenure = 0
missing_chgs

# Remove NA rows
df_raw <- df_raw %>%
  filter(!is.na(TotalCharges)) 

```
  
## Feature Engineering and Transformation
  
### Standardize flags to 0/1 and extract service details with one-hot encoding 
  
The dataset includes character flags which are transformed to numeric as required by neural networks. One-hot encoding is used for four columns with >2 values to preserve information about service types, and then the remaining binary Yes/No columns are simply converted to 0/1. Finally, the target variable is transformed to a factor to force the random forest models to classification rather than regression.

```{r features}

#----------------------
# 1. Manual one-hot encoding: Internet Service
#----------------------

# List values 
df_raw %>% 
  count(InternetService)

# Create new cols with binary flags
df_model <- df_raw %>%
  mutate(Internet_Svc = if_else(InternetService == "No", 0, 1),
         DSL    = if_else(InternetService == "DSL", 1, 0),
         Fiber  = if_else(InternetService == "Fiber optic", 1, 0))

# Compare new cols to original
df_model %>% 
  count(InternetService, Internet_Svc, DSL,Fiber)

# Remove original column
df_model <- df_model %>% 
  select(-InternetService)

#----------------------
# 2. Automated one-hot encoding: gender, contract, payment types
#----------------------

# List values for each column
df_model %>% 
  count(gender)

df_model %>%
  count(Contract)

df_model %>%
  count(PaymentMethod)

# One-hot encoding, removing original columns
df_model <- df_model %>%
  dummy_cols(
    select_columns = c("PaymentMethod", "Contract", "gender"),
    remove_selected_columns = TRUE
  )

#----------------------
# 3. Convert remaining binary flags to 0/1 (treat "No service" as 0)
#----------------------

df_model <- df_model %>% 
    mutate(PhoneService = if_else(PhoneService == "Yes", 1, 0),
           MultipleLines = if_else(MultipleLines == "Yes", 1, 0),
           OnlineSecurity = if_else(OnlineSecurity == "Yes", 1, 0),
           OnlineBackup = if_else(OnlineBackup == "Yes", 1, 0),
           DeviceProtection = if_else(DeviceProtection == "Yes", 1, 0),
           TechSupport = if_else(TechSupport == "Yes", 1, 0),
           StreamingTV = if_else(StreamingTV == "Yes", 1, 0),
           StreamingMovies = if_else(StreamingMovies == "Yes", 1, 0),
           Partner = if_else(Partner == "Yes", 1, 0),
           Dependents = if_else(Dependents == "Yes", 1, 0),
           PaperlessBilling = if_else(PaperlessBilling == "Yes", 1, 0),
           Churn = if_else(Churn == "Yes", 1, 0))

#----------------------
# Convert target variable to factor
#----------------------

df_model$Churn <- as.factor(df_model$Churn)

#----------------------
# Final dataframe 
#----------------------

glimpse(df_model)

#----------------------
# Summary stats for essay
#----------------------

summary_stats <- df_model %>% 
  summarize(
    `Customers` = n(),
    `Churn %` = mean(Churn == 1),
    `Mean Tenure` = mean(tenure, na.rm = TRUE),
    `Mean Monthly Charges` = mean(MonthlyCharges, na.rm = TRUE),
    `% Month to Month` = mean(`Contract_Month-to-month` == 1), 
    `% Phone Services` = mean(PhoneService == 1),
    `% Internet Services` = mean(Internet_Svc == 1)) %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

summary_stats 
```
  
## Modeling Preparation: Train/Test Data
  
### Initialize results matrix and create train/test split
  
A matrix is initialized to store metrics for each of our four models, and the dataset is split 70/30 into train and test data using a fixed seed for reproducibility. Customer ID is removed and the train and test datasets are compared to ensure the same one-hot encoded columns appear in both. Finally, column names are fixed to avoid issues with special characters in the random forest models.

```{r train_test}

# Initialize matrix to store metrics 
model_results <- matrix(nrow = 4, ncol = 5,
                        dimnames = list(
                          c("RF_default", "RF_tuned", "NN_simple", "NN_deep"),
                          c("Accuracy", "Precision", "Recall", "F1", "AUC")
                        ))

# Train/test split 
set.seed(123)

train_index <- sample(nrow(df_model), 0.7 * nrow(df_model))
df_train <- df_model[train_index, ]
df_test  <- df_model[-train_index, ]

# Drop customer ID
df_train <- df_train %>% select(-customerID)
df_test  <- df_test %>% select(-customerID)

# Add any missing one-hot encoded cols to df_test 
missing_cols <- setdiff(names(df_train), names(df_test))
df_test[missing_cols] <- 0
df_test <- df_test[, names(df_train)]

# Add any missing one-hot encoded cols to df_train 
missing_cols_rev <- setdiff(names(df_test), names(df_train))
df_train[missing_cols_rev] <- 0
df_train <- df_train[, names(df_test)]

# Fix col names with special characters (i.e. one hot encoded cols)
names(df_train) <- make.names(names(df_train))
names(df_test) <- make.names(names(df_test))
```
  
## Model #1: Random Forest with Default Parameters
  
The first model is a random forest with default parameters and 500 trees, with performance metrics calculated and stored in the results matrix. 

```{r rf_default, message=FALSE, warning=FALSE}

# Train Random Forest
rf_default_model <- randomForest(Churn ~ ., 
                         data = df_train, 
                         ntree = 500, 
                         importance = TRUE)

# Get predictions and probabilities
rf_default_preds <- predict(rf_default_model, newdata = df_test)
rf_default_probs <- predict(rf_default_model, newdata = df_test, type = "prob")[, 2]

# Confusion matrix using predictions
rf_default_cm <- confusionMatrix(rf_default_preds, df_test$Churn, positive = "1")
rf_default_cm

# Calculate AUC using probabilities
rf_default_roc <- roc(df_test$Churn, rf_default_probs)
rf_default_auc <- auc(rf_default_roc)

# Store results from confusion matrix and AUC 
model_results["RF_default", "Accuracy"]  <- rf_default_cm$overall["Accuracy"]
model_results["RF_default", "Precision"] <- rf_default_cm$byClass["Precision"]
model_results["RF_default", "Recall"]    <- rf_default_cm$byClass["Recall"]
model_results["RF_default", "F1"]        <- rf_default_cm$byClass["F1"]
model_results["RF_default", "AUC"]       <- rf_default_auc

# View results matrix
model_results
```
  
# Model #2: Random Forest with Tuned Parameters
  
To improve upon the first RF model, five-fold cross-validation was used to optimize the mtry value, or the number of predictors randomly selected at each split. Results were again stored in the results matrix for comparison.   

```{r rf_tuned, message=FALSE, warning=FALSE}

# Train with 5-fold cross-validation for mtry value
set.seed(42)

rf_tuned <- train(
  Churn ~ .,
  data = df_train,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 5
)

# Get predictions and probabilities
rf_tuned_preds <- predict(rf_tuned, newdata = df_test)
rf_tuned_probs <- predict(rf_tuned, newdata = df_test, type = "prob")[, 2]

# Confusion matrix using predictions
rf_tuned_cm <- confusionMatrix(rf_tuned_preds, df_test$Churn, positive = "1")
rf_tuned_cm

# Calculate AUC using probabilities
rf_tuned_roc <- roc(df_test$Churn, rf_tuned_probs)
rf_tuned_auc <- auc(rf_tuned_roc)

# Store results from CM and AUC in matrix
model_results["RF_tuned", "Accuracy"]    <- rf_tuned_cm$overall["Accuracy"]
model_results["RF_tuned", "Precision"]   <- rf_tuned_cm$byClass["Precision"]
model_results["RF_tuned", "Recall"]      <- rf_tuned_cm$byClass["Recall"]
model_results["RF_tuned", "F1"]          <- rf_tuned_cm$byClass["F1"]
model_results["RF_tuned", "AUC"]         <- rf_tuned_auc

# View updated matrix
model_results
```
  
# Model #3: Simple Neural Network
  
Next, a simple feedforward neural network with five hidden units was created using the nnet package. Multiple metrics were again evaluated and the results stored in the results matrix.

``` {r nn_simple}

# Fit simple neural network with 5 hidden units and light regularization
nn_simple <- nnet(
  Churn ~ .,
  data = df_train,
  size = 5,         
  decay = 0.01,     
  maxit = 200,
  trace = FALSE
)

# Predict on test data without the "Churn" column
nn_simple_preds <- predict(nn_simple, 
                           df_test %>% select(-Churn), 
                           type = "class")

# Confusion matrix (note: no AUC in nnet)
nn_simple_cm <- confusionMatrix(
  factor(nn_simple_preds, levels = c(0, 1)),
  df_test$Churn,
  positive = "1")

nn_simple_cm

# Store results from CM in matrix
model_results["NN_simple", "Accuracy"]     <- nn_simple_cm$overall["Accuracy"]
model_results["NN_simple", "Precision"]    <- nn_simple_cm$byClass["Precision"]
model_results["NN_simple", "Recall"]       <- nn_simple_cm$byClass["Recall"]
model_results["NN_simple", "F1"]           <- nn_simple_cm$byClass["F1"]
model_results["NN_simple", "AUC"]          <- NA  

# Display updated metrics
model_results
```
  
# Model #4: Deep Neural Network

Finally, a deep feedforward neural network with ten hidden units and stronger regularization is created in an attempt to improve the still-poor recall performance. 

``` {r nn_deep}

# Fit neural network with 10 hidden units and stronger regularization
nn_deep <- nnet(
  Churn ~ .,
  data = df_train,
  size = 10,       
  decay = 0.05,    
  maxit = 300,
  trace = FALSE
)

# Predict on test data without the "Churn" column
nn_deep_preds <- predict(nn_deep, df_test %>% select(-Churn), type = "class")

# Confusion matrix
nn_deep_cm <- confusionMatrix(
  factor(nn_deep_preds, levels = c(0, 1)),
  df_test$Churn,
  positive = "1")

nn_deep_cm

# Store metrics
model_results["NN_deep", "Accuracy"]     <- nn_deep_cm$overall["Accuracy"]
model_results["NN_deep", "Precision"]    <- nn_deep_cm$byClass["Precision"]
model_results["NN_deep", "Recall"]       <- nn_deep_cm$byClass["Recall"]
model_results["NN_deep", "F1"]           <- nn_deep_cm$byClass["F1"]
model_results["NN_deep", "AUC"]          <- NA  # no raw probs available

# Display updated metrics
model_results
```