---
title: "DATA 622 - Assignment 4 - Fox"
author: "Amanda Fox"
date: "2025-05-11"
output: html_document
---

  
## Introduction
  
This document contains all code for Assignment 4. The business goal is to predict customer churn for a fictional telecommunications company using the "Telco Customer Churn" dataset sourced from Kaggle. 

Two modeling approaches are used: random forest and neural networks, with both default and tuned versions of each. All models are evaluated on multiple metrics, which are stored in a matrix for comparison. Balanced performance is important but recall is prioritized somewhat, reflecting the desire to capture as many at-risk customers as possible due to the high cost of a lost customer. 
  
## Load libraries
  
```{r libraries, message=FALSE, warning=FALSE}

library(tidyverse)
library(fastDummies)
library(randomForest)
library(pROC)
library(caret)
library(nnet)
library(scales)
```


## Data Prep

### Load data

```{r load, message=FALSE, warning=FALSE}

# Load data
df_raw <- read_csv("https://raw.githubusercontent.com/AmandaSFox/DATA622/refs/heads/main/Assignment_4/WA_Fn-UseC_-Telco-Customer-Churn.csv")

# View raw dataset
glimpse(df_raw)
```
  
### Handle missing values and duplicate records 

This dataset is preprocessed and very clean: no duplicate records were found and only 11 NA values. All NA values were in the "Total Charges" column and associated with brand new accounts. These were removed from the dataset as not useful for churn analysis.
  
```{r clean}

# Duplicate rows
sum(duplicated(df_raw))

# Count NA values by column
colSums(is.na(df_raw)) 

# View NA data
missing_chgs <- df_raw %>% 
  filter(is.na(TotalCharges)) # All have tenure = 0
missing_chgs

# Remove NA rows
df_raw <- df_raw %>%
  filter(!is.na(TotalCharges)) 

```
  
## Feature Engineering and Transformation
  
### Standardize flags to numeric 0/1 and extract service type details 
  
The dataset includes character flags which are transformed to numeric (0/1) as required by neural networks. One-hot encoding was used for four columns with >2 values to preserve information about service types, and the remaining binary columns were converted to 0/1 numeric columns. 

Finally, the target variable is transformed to a factor to force the random forest models to classification rather than regression.

```{r features}

#----------------------
# 1. Manual one-hot encoding: Internet Service
#----------------------

# List values 
df_raw %>% 
  count(InternetService)

# Create new cols with binary flags
df_model <- df_raw %>%
  mutate(Internet_Svc = if_else(InternetService == "No", 0, 1),
         DSL    = if_else(InternetService == "DSL", 1, 0),
         Fiber  = if_else(InternetService == "Fiber optic", 1, 0))

# Compare new cols to original
df_model %>% 
  count(InternetService, Internet_Svc, DSL,Fiber)

# Remove original column
df_model <- df_model %>% 
  select(-InternetService)

#----------------------
# 2. Automated one-hot encoding: gender, contract, payment types
#----------------------

# List values for each column
df_model %>% 
  count(gender)

df_model %>%
  count(Contract)

df_model %>%
  count(PaymentMethod)

# One-hot encoding, removing original columns
df_model <- df_model %>%
  dummy_cols(
    select_columns = c("PaymentMethod", "Contract", "gender"),
    remove_selected_columns = TRUE
  )

#----------------------
# 3. Convert remaining binary flags to 0/1 (treat "No service" as 0)
#----------------------

df_model <- df_model %>% 
    mutate(Phone_Service = if_else(PhoneService == "Yes", 1, 0),
           MultipleLines = if_else(MultipleLines == "Yes", 1, 0),
           OnlineSecurity = if_else(OnlineSecurity == "Yes", 1, 0),
           OnlineBackup = if_else(OnlineBackup == "Yes", 1, 0),
           DeviceProtection = if_else(DeviceProtection == "Yes", 1, 0),
           TechSupport = if_else(TechSupport == "Yes", 1, 0),
           StreamingTV = if_else(StreamingTV == "Yes", 1, 0),
           StreamingMovies = if_else(StreamingMovies == "Yes", 1, 0),
           Partner = if_else(Partner == "Yes", 1, 0),
           Dependents = if_else(Dependents == "Yes", 1, 0),
           PaperlessBilling = if_else(PaperlessBilling == "Yes", 1, 0),
           Churn = if_else(Churn == "Yes", 1, 0))

glimpse(df_model)

#----------------------
# Convert target variable to factor
#----------------------

df_model$Churn <- as.factor(df_model$Churn)

#----------------------
# Final dataframe 
#----------------------

glimpse(df_model)

#----------------------
# Summary stats for essay 
#----------------------

summary_stats <- df_model %>% 
  summarize(
    `Customers` = n(),
    `Churn %` = mean(Churn == 1),
    `Mean Tenure` = mean(tenure, na.rm = TRUE),
    `Mean Monthly Charges` = mean(MonthlyCharges, na.rm = TRUE),
    `% Month to Month` = mean(`Contract_Month-to-month` == 1), 
    `% Phone Services` = mean(Phone_Svc == 1),
    `% Internet Services` = mean(Internet_Svc == 1)) %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "Value")

summary_stats 
```
  
## Modeling Preparation: Train/Test Data
  
### Initialize results matrix and create train/test split
  
A matrix is initialized to store metrics for each of our four models, and the dataset is split 70/30 into train and test data using a fixed seed for reproducibility. Customer ID was removed. The train and test data was compared to ensure the same one-hot encoded columns appeared in both, and those columns were renamed to avoid issues with special characters.

```{r train_test}

# Initialize matrix to store metrics 
model_results <- matrix(nrow = 4, ncol = 7,
                        dimnames = list(
                          c("RF_default", "RF_tuned", "NN_simple", "NN_deep"),
                          c("Accuracy", "Precision", "Recall", "F1", "AUC", "Sensitivity","Specificity")
                        ))

# Train/test split 
set.seed(123)

train_index <- sample(nrow(df_model), 0.7 * nrow(df_model))
df_train <- df_model[train_index, ]
df_test  <- df_model[-train_index, ]

# Drop customer ID
df_train <- df_train %>% select(-customerID)
df_test  <- df_test %>% select(-customerID)

# Add any missing one-hot encoded cols to df_test 
missing_cols <- setdiff(names(df_train), names(df_test))
df_test[missing_cols] <- 0
df_test <- df_test[, names(df_train)]

# Add any missing one-hot encoded cols to df_train 
missing_cols_rev <- setdiff(names(df_test), names(df_train))
df_train[missing_cols_rev] <- 0
df_train <- df_train[, names(df_test)]

# Fix col names with special characters (i.e. one hot encoded cols)
names(df_train) <- make.names(names(df_train))
names(df_test) <- make.names(names(df_test))
```
  
## Model #1: Random Forest with Default Parameters
  
The first model was a random forest with default parameters and 500 trees, with metrics stored in the results matrix. 

```{r rf_default}

# Train Random Forest
rf_default_model <- randomForest(Churn ~ ., 
                         data = df_train, 
                         ntree = 500, 
                         importance = TRUE)

# Get predictions and probabilities
rf_default_preds <- predict(rf_default_model, newdata = df_test)
rf_default_probs <- predict(rf_default_model, newdata = df_test, type = "prob")[, 2]

# Confusion matrix using predictions
rf_default_cm <- confusionMatrix(rf_default_preds, df_test$Churn, positive = "1")
rf_default_cm

# Calculate AUC using probabilities
rf_default_roc <- roc(df_test$Churn, rf_default_probs)
rf_default_auc <- auc(rf_default_roc)

# Store results from confusion matrix and AUC 
model_results["RF_default", "Accuracy"]  <- rf_default_cm$overall["Accuracy"]
model_results["RF_default", "Precision"] <- rf_default_cm$byClass["Precision"]
model_results["RF_default", "Recall"]    <- rf_default_cm$byClass["Recall"]
model_results["RF_default", "F1"]        <- rf_default_cm$byClass["F1"]
model_results["RF_default", "AUC"]       <- rf_default_auc
model_results["RF_default", "Sensitivity"] <- rf_default_cm$byClass["Sensitivity"]
model_results["RF_tuned", "Specificity"] <- rf_default_cm$byClass["Specificity"]

# View results matrix
model_results
```
  
# Model #2: Random Forest with Tuned Parameters
  
To improve upon the first model, five-fold cross-validation was used to optimize the mtry value, and results again stored in the results matrix.   

```{r rf_tuned}

# Train with 5-fold cross-validation for mtry value
set.seed(42)

rf_tuned <- train(
  Churn ~ .,
  data = df_train,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 5
)

# Get predictions and probabilities
rf_tuned_preds <- predict(rf_tuned, newdata = df_test)
rf_tuned_probs <- predict(rf_tuned, newdata = df_test, type = "prob")[, 2]

# Confusion matrix using predictions
rf_tuned_cm <- confusionMatrix(rf_tuned_preds, df_test$Churn, positive = "1")
rf_tuned_cm

# Calculate AUC using probabilities
rf_tuned_roc <- roc(df_test$Churn, rf_tuned_probs)
rf_tuned_auc <- auc(rf_tuned_roc)

# Store results from CM and AUC in matrix
model_results["RF_tuned", "Accuracy"]    <- rf_tuned_cm$overall["Accuracy"]
model_results["RF_tuned", "Precision"]   <- rf_tuned_cm$byClass["Precision"]
model_results["RF_tuned", "Recall"]      <- rf_tuned_cm$byClass["Recall"]
model_results["RF_tuned", "F1"]          <- rf_tuned_cm$byClass["F1"]
model_results["RF_tuned", "AUC"]         <- rf_tuned_auc
model_results["RF_tuned", "Sensitivity"] <- rf_tuned_cm$byClass["Sensitivity"]
model_results["RF_tuned", "Specificity"] <- rf_tuned_cm$byClass["Specificity"]

# View updated matrix
model_results
```
  
# Model #3: Simple Neural Network



# Convert Churn back to numeric 
y_train <- as.numeric(df_train$Churn)
y_test <- as.numeric(df_test$Churn)

# Drop target column and convert to matrix
x_train <- df_train %>% select(-Churn) %>% as.matrix()
x_test  <- df_test %>% select(-Churn) %>% as.matrix()

# Build model
nn_simple <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dense(units = 1, activation = "sigmoid")

# Compile
nn_simple %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

# Fit model
history <- nn_simple %>% fit(
  x = x_train,
  y = y_train,
  epochs = 30,
  batch_size = 32,
  validation_split = 0.2,
  verbose = 0
)

# Predict
nn_simple_probs <- predict(nn_simple, x_test)
nn_simple_preds <- ifelse(nn_simple_probs > 0.5, 1, 0)

# Evaluate
nn_simple_cm <- confusionMatrix(
  factor(nn_simple_preds, levels = c(0, 1)),
  factor(y_test, levels = c(0, 1)),
  positive = "1"
)
nn_simple_roc <- roc(y_test, nn_simple_probs)
nn_simple_auc <- auc(nn_simple_roc)

# Store metrics
model_results["NN_simple", "Accuracy"]    <- nn_simple_cm$overall["Accuracy"]
model_results["NN_simple", "Precision"]   <- nn_simple_cm$byClass["Precision"]
model_results["NN_simple", "Recall"]      <- nn_simple_cm$byClass["Recall"]
model_results["NN_simple", "F1"]          <- nn_simple_cm$byClass["F1"]
model_results["NN_simple", "AUC"]         <- nn_simple_auc
model_results["NN_simple", "Sensitivity"] <- nn_simple_cm$byClass["Sensitivity"]
model_results["NN_simple", "Specificity"] <- nn_simple_cm$byClass["Specificity"]
